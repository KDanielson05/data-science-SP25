---
title: "Regression Case Study: PSAAP II"
author: "Katherine Danielson"
date: 04-15-2025
output:
  github_document:
    toc: true
---

*Purpose*: Confidence and prediction intervals are useful for studying "pure sampling" of some distribution. However, we can combine CI and PI with regression analysis to equip our modeling efforts with powerful notions of uncertainty. In this challenge, you will use fluid simulation data in a regression analysis with uncertainty quantification (CI and PI) to support engineering design.

<!-- include-rubric -->

# Grading Rubric

<!-- -------------------------------------------------- -->

Unlike exercises, **challenges will be graded**. The following rubrics define how you will be graded, both on an individual and team basis.

## Individual

<!-- ------------------------- -->

| Category | Needs Improvement | Satisfactory |
|------------------------|------------------------|------------------------|
| Effort | Some task **q**'s left unattempted | All task **q**'s attempted |
| Observed | Did not document observations, or observations incorrect | Documented correct observations based on analysis |
| Supported | Some observations not clearly supported by analysis | All observations clearly supported by analysis (table, graph, etc.) |
| Assessed | Observations include claims not supported by the data, or reflect a level of certainty not warranted by the data | Observations are appropriately qualified by the quality & relevance of the data and (in)conclusiveness of the support |
| Specified | Uses the phrase "more data are necessary" without clarification | Any statement that "more data are necessary" specifies which *specific* data are needed to answer what *specific* question |
| Code Styled | Violations of the [style guide](https://style.tidyverse.org/) hinder readability | Code sufficiently close to the [style guide](https://style.tidyverse.org/) |

## Submission

<!-- ------------------------- -->

Make sure to commit both the challenge report (`report.md` file) and supporting files (`report_files/` folder) when you are done! Then submit a link to Canvas. **Your Challenge submission is not complete without all files uploaded to GitHub.**

```{r setup}
library(tidyverse)
library(modelr)
library(broom)

## Helper function to compute uncertainty bounds
add_uncertainties <- function(data, model, prefix = "pred", ...) {
  df_fit <-
    stats::predict(model, data, ...) %>%
    as_tibble() %>%
    rename_with(~ str_c(prefix, "_", .))

  bind_cols(data, df_fit)
}
```

# Orientation: Exploring Simulation Results

*Background*: The data you will study in this exercise come from a computational fluid dynamics (CFD) [simulation campaign](https://www.sciencedirect.com/science/article/abs/pii/S0301932219308651?via%3Dihub) that studied the interaction of turbulent flow and radiative heat transfer to fluid-suspended particles[1]. These simulations were carried out to help study a novel design of [solar receiver](https://en.wikipedia.org/wiki/Concentrated_solar_power), though they are more aimed at fundamental physics than detailed device design. The following code chunk downloads and unpacks the data to your local `./data/` folder.

```{r data-download-unzip}
## NOTE: No need to edit this chunk
## Download PSAAP II data and unzip
url_zip <- "https://ndownloader.figshare.com/files/24111269"
filename_zip <- "./data/psaap.zip"
filename_psaap <- "./data/psaap.csv"

curl::curl_download(url_zip, destfile = filename_zip)
unzip(filename_zip, exdir = "./data")
df_psaap <- read_csv(filename_psaap)
```

![PSAAP II irradiated core flow](./images/psaap-setup.png) Figure 1. An example simulation, frozen at a specific point in time. An initial simulation is run (HIT SECTION) to generate a turbulent flow with particles, and that swirling flow is released into a rectangular domain (RADIATED SECTION) with bulk downstream flow (left to right). Concentrated solar radiation transmits through the optically transparent fluid, but deposits heat into the particles. The particles then convect heat into the fluid, which heats up the flow. The false-color image shows the fluid temperature: Notice that there are "hot spots" where hot particles have deposited heat into the fluid. The dataset `df_psaap` gives measurements of `T_norm = (T - T0) / T0` averaged across planes at various locations along the RADIATED SECTION.

### **q1** Perform your "initial checks" to get a sense of the data.

```{r q1-task}
## TODO: Perform your initial checks
glimpse(df_psaap)               # Quick overview of data types and structure
summary(df_psaap)               # Statistical summary of numeric columns
sapply(df_psaap, function(x) sum(is.na(x)))  # Count missing values per column

```

**Observations**:

-   As seen from the glimpse, `df_psaap` has 22 columns and 140 rows.

    -   Within these columns, there are 22 distinct variables and 4 categories they fall into: spatial, metadata, input and output. (Reference the table below to learn more about what each variable means/is).

-   There were a total of 35 modeling simulations as defined by the variable `idx` – these are labeled with numbers 1-35.

-   There are no missing values/NAs in any of the variables.

-   Most variables appear to be numeric and continuous (ex. `mu_f`, `eps_p`, `d_p`), with ranges and quartiles that suggest some skew.

    -   Potential skewed variables: - `mu_f`: Wide range, max (±370,000+), indicating possible outliers or high variance.
    -   `d_p`: Small values in scientific notation, may require log transformation if used in modeling.
    -   `W`: Includes negative values and appears to span a large range.

-   Variables like `mu_f`, `I_0`, `eps_p`, `W`, and `d_p` may contain extreme outliers based on their range vs. interquartile spread.

    -   Example: `mu_f` has a median of \~2.02 but a max of 370.5

-   There are additionally a lot of scale differences between variables – `W` is in the tens, `mu_f` in thousands, `eps_p` less than 1.

The important variables in this dataset are:

| Variable | Category | Meaning                           |
|----------|----------|-----------------------------------|
| `x`      | Spatial  | Channel location                  |
| `idx`    | Metadata | Simulation run                    |
| `L`      | Input    | Channel length                    |
| `W`      | Input    | Channel width                     |
| `U_0`    | Input    | Bulk velocity                     |
| `N_p`    | Input    | Number of particles               |
| `k_f`    | Input    | Turbulence level                  |
| `T_f`    | Input    | Fluid inlet temp                  |
| `rho_f`  | Input    | Fluid density                     |
| `mu_f`   | Input    | Fluid viscosity                   |
| `lam_f`  | Input    | Fluid conductivity                |
| `C_fp`   | Input    | Fluid isobaric heat capacity      |
| `rho_p`  | Input    | Particle density                  |
| `d_p`    | Input    | Particle diameter                 |
| `C_pv`   | Input    | Particle isochoric heat capacity  |
| `h`      | Input    | Convection coefficient            |
| `I_0`    | Input    | Radiation intensity               |
| `eps_p`  | Input    | Radiation absorption coefficient  |
| `avg_q`  | Output   | Plane-averaged heat flux          |
| `avg_T`  | Output   | Plane-averaged fluid temperature  |
| `rms_T`  | Output   | Plane-rms fluid temperature       |
| `T_norm` | Output   | Normalized fluid temperature rise |

The primary output of interest is `T_norm = (avg_T - T_f) / T_f`, the normalized (dimensionless) temperature rise of the fluid, due to heat transfer. These measurements are taken at locations `x` along a column of fluid, for different experimental settings (e.g. different dimensions `W, L`, different flow speeds `U_0`, etc.).

### **q2** Visualize `T_norm` against `x`. Note that there are multiple simulations at different values of the Input variables: Each simulation result is identified by a different value of `idx`.

```{r q2-task}
## TODO: Visualize the data in df_psaap with T_norm against x;
##       design your visual to handle the multiple simulations,
##       each identified by different values of idx


ggplot(df_psaap, aes(x = x, y = T_norm, group = idx, color = as.factor(idx))) +
  geom_line(alpha = 0.6) +
  labs(
    title = "T_norm vs. x for Multiple Simulations",
    x = "x",
    y = "Normalized Temperature (T_norm)",
    color = "Simulation ID (idx)"
  ) +
  theme_minimal() +
  theme(legend.position = "none")  # Hide legend if too many simulations
```

## Modeling

The following chunk will split the data into training and validation sets.

```{r data-split}
## NOTE: No need to edit this chunk
# Addl' Note: These data are already randomized by idx; no need
# to additionally shuffle the data!
df_train <- df_psaap %>% filter(idx %in% 1:20)
df_validate <- df_psaap %>% filter(idx %in% 21:36)
```

One of the key decisions we must make in modeling is choosing predictors (features) from our observations to include in the model. Ideally we should have some intuition for why these predictors are reasonable to include in the model; for instance, we saw above that location along the flow `x` tends to affect the temperature rise `T_norm`. This is because fluid downstream has been exposed to solar radiation for longer, and thus is likely to be at a higher temperature.

Reasoning about our variables---at least at a *high level*---can help us to avoid including *fallacious* predictors in our models. You'll explore this idea in the next task.

### **q3** The following code chunk fits a few different models. Compute a measure of model accuracy for each model on `df_validate`, and compare their performance.

```{r q3-task}
## NOTE: No need to edit these models
fit_baseline <- 
  df_train %>% 
  lm(formula = T_norm ~ x)

fit_cheat <- 
  df_train %>% 
  lm(formula = T_norm ~ avg_T)

fit_nonphysical <- 
  df_train %>% 
  lm(formula = T_norm ~ idx)

## TODO: Compute a measure of accuracy for each fit above;
##       compare their relative performance
mse(fit_baseline, df_train)
mse(fit_cheat, df_train)
mse(fit_nonphysical, df_train)

```

**Observations**:

-   Which model is *most accurate*? Which is *least accurate*?
    -   Based on the printed MSE values, the model with the lowest MSE is the most accurate. Thus, `fit_cheat` has the lowest MSE (0.05194415), making it the most accurate while `fit_nonphysical` has the highest MSE (0.2322454), making it the least accurate.
-   What *Category* of variable is `avg_T`? Why is it such an effective predictor?
    -   `avg_T` falls into the category of an output variable–it is a continuous numeric variable. It’s highly effective because it's closely related to or derived from the target variable `T_norm`, meaning it shares a strong underlying relationship and is thus a highly effective predictor.
-   Would we have access to `avg_T` if we were trying to predict a *new* value of `T_norm`? Is `avg_T` a valid predictor?
    -   We would not have access to `avg_T` when making a real prediction for a new value of `T_norm`, as it is typically calculated from simulation results; thus, the simulation would have to be run to gain the values of `avg_T`. Therefore, it's not a valid predictor in a realistic or forward-looking modeling context.
-   What *Category* of variable is `idx`? Does it have any physical meaning?
    -   `idx` is a belongs to the metadata category and doesn't represent a meaningful physical variable–it appears to be integer identifier and indicates the simulation number. As such, using it to predict `T_norm` doesn't reflect any underlying physical relationship and makes the model overfit to specific runs. This reflects part of the reason as to why it has the highest `mse` value and is the least accurate predictor.

### **q4** Interpret this model

Interpret the following model by answering the questions below.

*Note*. The `-` syntax in R formulas allows us to exclude columns from fitting. So `T_norm ~ . - x` would fit on all columns *except* `x`.

```{r q4-task}
## TODO: Inspect the regression coefficients for the following model
fit_q4 <- 
  df_train %>% 
  lm(formula = T_norm ~ . - idx - avg_q - avg_T - rms_T)

fit_q4 %>% tidy()
df_psaap %>% 
  summarize(
    sd_x = sd(x),
    sd_tf = sd(T_f)
  )
```

**Observations**:

-   Which columns are excluded in the model formula above? What categories do these belong to? Why are these important quantities to leave out of the model?
    -   The excluded columns are: `idx`, `avg_q`, `avg_T`, and `rms_T`.
    -   Categories:
        -   `idx`: belongs to the metadata category and doesn't represent a meaningful physical variable
        -   `avg_q`, `avg_T`, and `rms_T` are all output variables and are aggregate and/or derived measures that are closely related to `T_norm`.
    -   These variables (`idx`, `avg_q`, `avg_T`, and `rms_T`) are excluded as for two main reasons. `idx` is excluded as it has no physical meaning–it labels simulations and could cause overfitting. On the other hand, `avg_q`, `avg_T`, and `rms_T` are excluded as they are summary/post-process variables that are too heavily related to `T_norm`. They could hinder interpretation of the real/physical outputs.
-   Which inputs are *statistically significant*, according to the model?
    -   Assuming a statistically significant threshold of a p-value of 0.05, the terms `x`, `L`, `W`, `U_0`, `C_fp`, `d_p` and `I_0` are all statistically significant.
-   What is the regression coefficient for `x`? What about the regression coefficient for `T_f`?
    -   The regression coefficient for x is illustrated by the "estimate" column in the tidy table. For `x`, the regression coefficient is 1.018323 while the regression coefficient for `T_f` is -0.0003791436.
-   What is the standard deviation of `x` in `df_psaap`? What about the standard deviation of `T_f`?
    -   The standard deviation of `x` in `df_psaap` is 0.2805121 and the standard deviation of `T_f` is 38.94204.
-   How do these standard deviations relate to the regression coefficients for `x` and `T_f`?
    -   For `x`, although the regression coefficient is relatively high (1.018323), the small standard deviation (0.2805121) implies that `x` does not vary much in the data set. As a result, even a one-standard-deviation change in x only causes a change of approximately 0.286 units in. Comparatively, for T_f, the regression coefficient is very small ( − 0.000379) on a per-unit basis. However, `T_f` exhibits a much larger variability (standard deviation of 38.94204), so the effect of a one-standard-deviation change is about -0.015 units. In this case, `x` has a moderate overall impact despite a small range, while `T_f`, even with a tiny per-unit effect, does not contribute much when scaled by its large variability
-   Note that literally *all* of the inputs above have *some* effect on the output `T_norm`; so they are all "significant" in that sense. What does this tell us about the limitations of statistical significance for interpreting regression coefficients?
    -   Statistical significance (i.e., a p-value below 0.05) indicates that a predictor's effect is unlikely to be due to random chance, but it does not necessarily imply that the effect is practically or clinically important. With large sample sizes, even very small effects can be statistically significant. Conversely, a predictor might be practically important even if its p-value is slightly above 0.05, as seen with `N_p` (p ≈ 0.0533). This illustrates that statistical significance is influenced by sample size and variability, and should be interpreted along with effect size (i.e., the magnitude of the coefficient) and domain-specific considerations. P-values do not convey practical impact or importance. Thus, while statistical significance is a useful indicator for hypothesis testing, it must be supplemented with a thoughtful evaluation of effect sizes and real-world relevance in the context of the model. To see if a predictor has a true impact, understanding the standard deviation in coordination with p-value is better for understanding impact of regression coefficients.

## Contrasting CI and PI

Let's revisit the ideas of confidence intervals (CI) and prediction intervals (PI). Let's fit a very simple model to these data, one which only considers the channel location and ignores all other inputs. We'll also use the helper function `add_uncertainties()` (defined in the `setup` chunk above) to add approximate CI and PI to the linear model.

```{r data-simple-model}
## NOTE: No need to edit this chunk
fit_simple <-
  df_train %>%
  lm(data = ., formula = T_norm ~ x)

df_intervals <-
  df_train %>%
  add_uncertainties(fit_simple, interval = "confidence", prefix = "ci") %>%
  add_uncertainties(fit_simple, interval = "prediction", prefix = "pi")
```

The following figure visualizes the regression CI and PI against the objects they are attempting to capture:

```{r data-simple-model-vis}
## NOTE: No need to edit this chunk
df_intervals %>%
  select(T_norm, x, matches("ci|pi")) %>%
  pivot_longer(
    names_to = c("method", ".value"),
    names_sep = "_",
    cols = matches("ci|pi")
  ) %>%

  ggplot(aes(x, fit)) +
  geom_errorbar(
    aes(ymin = lwr, ymax = upr, color = method),
    width = 0.05,
    size = 1
  ) +
  geom_smooth(
    data = df_psaap %>% mutate(method = "ci"),
    mapping = aes(x, T_norm),
    se = FALSE,
    linetype = 2,
    color = "black"
   ) +
  geom_point(
    data = df_validate %>% mutate(method = "pi"),
    mapping = aes(x, T_norm),
    size = 0.5
  ) +

  facet_grid(~method) +
  theme_minimal() +
  labs(
    x = "Channel Location (-)",
    y = "Normalized Temperature Rise (-)"
  )
```

Under the `ci` facet we have the regression confidence intervals and the mean trend (computed with all the data `df_psaap`). Under the `pi` facet we have the regression prediction intervals and the `df_validation` observations.

**Punchline**:

-   Confidence intervals are meant to capture the *mean trend*
-   Prediction intervals are meant to capture *new observations*

Both CI and PI are a quantification of the uncertainty in our model, but the two intervals designed to answer different questions.

Since CI and PI are a quantification of uncertainty, they should tend to *narrow* as our model becomes more confident in its predictions. Building a more accurate model will often lead to a reduction in uncertainty. We'll see this phenomenon in action with the following task:

### **q5** The following code will construct a predicted-vs-actual plot with your model from *q4* and add prediction intervals. Study the results and answer the questions below under *observations*.

```{r q5-task}
## TODO: Run this code and interpret the results
## NOTE: No need to edit this chunk
## NOTE: This chunk will use your model from q4; it will predict on the
##       validation data, add prediction intervals for every prediction,
##       and visualize the results on a predicted-vs-actual plot. It will
##       also compare against the simple `fit_simple` defined above.
bind_rows(
  df_psaap %>% 
    add_uncertainties(fit_simple, interval = "prediction", prefix = "pi") %>% 
    select(T_norm, pi_lwr, pi_fit, pi_upr) %>% 
    mutate(model = "x only"),
  df_psaap %>% 
    add_uncertainties(fit_q4, interval = "prediction", prefix = "pi") %>% 
    select(T_norm, pi_lwr, pi_fit, pi_upr) %>% 
    mutate(model = "q4"),
) %>% 
  
  ggplot(aes(T_norm, pi_fit)) +
  geom_abline(slope = 1, intercept = 0, color = "grey80", size = 2) +
  geom_errorbar(
    aes(ymin = pi_lwr, ymax = pi_upr),
    width = 0
  ) +
  geom_point() +
  
  facet_grid(~ model, labeller = label_both) +
  theme_minimal() +
  labs(
    title = "Predicted vs Actual",
    x = "Actual T_norm",
    y = "Predicted T_norm"
  )
```

**Observations**:

-   Which model tends to be more accurate? How can you tell from this predicted-vs-actual plot?
    -   The model that includes q4 rather than x only is much more accurate. In the q4 model, the predicted values and their prediction intervals are much close to the actual values. Comparatively, the x only model has much wider prediction intervals and their predicted values follow the actual trends much less. This is especially seen as the actual and predicted values get higher. The x only model appears to have a cap slightly above 1 in the predicted values while the actual continues to increase.
-   Which model tends to be *more confident* in its predictions? Put differently, which model has *narrower prediction intervals*?
    -   The q4 model is much more confident in its predictions. That is, the prediction intervals are much narrower compared to the x only model.
-   How many predictors does the `fit_simple` model need in order to make a prediction? What about your model `fit_q4`?
    -   `fit_simple` is a model that only uses x for its prediction–it uses one predictor. Comparatively, `fit_q4` uses all variables minus `idx`, `avg_q`, `avg_T`, and `rms_T`. Thus, `fit_q4` uses 17 predictor values and thus is much more accurate and confident in its predictions.

Based on these results, you might be tempted to always throw every reasonable variable into the model. For some cases, that might be the best choice. However, some variables might be *outside our control*; for example, variables involving human behavior cannot be fully under our control. Other variables may be *too difficult to measure*; for example, it is *in theory* possible to predict the strength of a component by having detailed knowledge of its microstructure. However, it is *patently infeasible* to do a detailed study of *every single component* that gets used in an airplane.

In both cases---human behavior and variable material properties---we would be better off treating those quantities as random variables. There are at least two ways we could treat these factors: 1. Explicitly model some inputs as random variables and construct a model that *propagates* that uncertainty from inputs to outputs, or 2. Implicitly model the uncontrolled the uncontrolled variables by not including them as predictors in the model, and instead relying on the error term $\epsilon$ to represent these unaccounted factors. You will pursue strategy 2. in the following Case Study.

# Case Study: Predicting Performance Ranges

### **q6** You are consulting with a team that is designing a prototype heat transfer device. They are asking you to help determine a *dependable range of values* for `T_norm` they can design around for this *single prototype*. The realized value of `T_norm` must not be too high as it may damage the downstream equipment, but it must also be high enough to extract an acceptable amount of heat.

In order to maximize the conditions under which this device can operate successfully, the design team has chosen to fix the variables listed in the table below, and consider the other variables to fluctuate according to the values observed in `df_psaap`.

| Variable | Value    |
|----------|----------|
| `x`      | 1.0      |
| `L`      | 0.2      |
| `W`      | 0.04     |
| `U_0`    | 1.0      |
| (Other)  | (Varies) |

Your task is to use a regression analysis to deliver to the design team a *dependable range* of values for `T_norm`, given their proposed design, and at a fairly high level `0.8`. Perform your analysis below (use the helper function `add_uncertainties()` with the `level` argument!), and answer the questions below.

*Hint*: This problem will require you to *build a model* by choosing the appropriate variables to include in the analysis. Think about *which variables the design team can control*, and *which variables they have chosen to allow to vary*. You will also need to choose between computing a CI or PI for the design prediction.

```{r q6-task}
# NOTE: No need to change df_design; this is the target the client
#       is considering
df_design <- tibble(x = 1, L = 0.2, W = 0.04, U_0 = 1.0)
# NOTE: This is the level the "probability" level customer wants
pr_level <- 0.8

## TODO: Fit a model, assess the uncertainty in your prediction, 
#        use the validation data to check your uncertainty estimates, and 
#        make a recommendation on a *dependable range* of values for T_norm
#        at the point `df_design`
fit_q6 <- 
  df_train %>% 
  lm(formula = T_norm ~ x + L + W + U_0)

df_design_pred <- 
  df_design %>% 
  add_uncertainties(
    fit_q6,
    interval = "prediction",
    level = pr_level, # this is 0.8,
    prefix = "pi"
  ) 

df_valid_pred <- 
  df_validate %>% 
  add_uncertainties(
    fit_q6,
    interval = "prediction",
    level = pr_level,
    prefix = "pi"
  )

df_valid_pred %>% 
  summarize(
    fraction_in = mean((pi_lwr <= T_norm) & (T_norm <= pi_upr))
  )

df_design_pred %>% 
  select(pi_lwr, pi_fit, pi_upr)


```

**Recommendation**:

-   How much do you trust your model? Why?
    -   I trust my model loosely but believe it could be improved. With the validation data, 93.33% of all predicted data points are in the range. However, as they are trying for only one prototype rather than a group and being too high or low with the `T_norm` could fully ruin the prototype, it would be beneficial if the `fraction_in` was higher.
-   What kind of interval---confidence or prediction---would you use for this task, and why?
    -   I would use a prediction interval because the team is interested in the likely range of individual future outcomes of `T_norm`, not the average/mean like the confidence interval would. This accounts for both model uncertainty and inherent variation in future data.
-   What fraction of validation cases lie within the intervals you predict? (NB. Make sure to calculate your intervals *based on the validation data*; don't just use one single interval!) How does this compare with `pr_level`?
    -   As illustrated by the `fraction_in` variable, 93.33% of the validation cases lie within the intervals. Comparatively, this is higher than the `pr_level` of 0.8 or 80%.
-   What interval for `T_norm` would you recommend the design team to plan around?
    -   One such recommendation for planning could be to utilize the prediction intervals created from the model. This would place the lower interval of `T_norm` at 1.45685 and the upper at 2.296426. To make it slightly more accurate, it may be a beneficial idea to shrink these boundaries even further.
-   Are there any other recommendations you would provide?
    -   One recommendation would be to increase the `pr_level` to 0.9 or even 0.95 to make the `pi_lwr` and `pi_upr` ranges smaller and increase the `fraction_in` percentage. This would allow for an increasingly accurate design where there would be a higher trust in the model. Another method I would suggest would be model refinement. Explore additional variables or interactions that might improve the model's accuracy. Conduct further analysis to identify any potential improvements. This could look like fixing more variables in `df_design` so that tighter control and model accuracy could be achieved. Additionally, including regular validation would be beneficial as well. Continuously validating the model with new data to ensure its reliability over time. This will help in maintaining the accuracy and dependability of the predictions.

*Bonus*: One way you could take this analysis further is to recommend which other variables the design team should tightly control. You could do this by fixing values in `df_design` and adding them to the model. An exercise you could carry out would be to systematically test the variables to see which ones the design team should more tightly control.

# References

-   [1] Jofre, del Rosario, and Iaccarino "Data-driven dimensional analysis of heat transfer in irradiated particle-laden turbulent flow" (2020) *International Journal of Multiphase Flow*, <https://doi.org/10.1016/j.ijmultiphaseflow.2019.103198>
